# Design Decisions

This document explains the key architectural and technology choices made during Strands CLI development, the rationale behind each decision, and the trade-offs considered.

## Why YAML/JSON Workflow Specifications?

### The Decision

Strands CLI uses **YAML/JSON files** for workflow specifications instead of Python code, DSLs, or GUI builders.

### Rationale

**1. Declarative over Imperative**
- **Declarative**: Describe *what* you want (agents, steps, dependencies)
- **Imperative**: Describe *how* to achieve it (loops, conditionals, state management)

Users focus on workflow structure, not orchestration logic.

**2. Version Control & Code Review**
YAML files integrate seamlessly into Git workflows:
```bash
git diff workflow.yaml  # See what changed
git blame workflow.yaml  # Who made this change?
```

**3. Cross-Language & Cross-Tool Compatibility**
YAML/JSON specs can be:
- Generated by other tools (UI builders, CI/CD pipelines)
- Consumed by other tools (validators, visualizers)
- Edited by non-programmers (product managers, analysts)

**4. Static Analysis**
JSON Schema validation enables:
- Pre-execution validation (catch errors before LLM calls)
- IDE autocomplete and inline documentation
- Automated testing of workflow specs

**5. Security Sandboxing**
YAML specs are **data**, not **code**:
- No arbitrary code execution (unlike Python workflows)
- Template sandboxing (Jinja2 SandboxedEnvironment)
- Tool allowlisting enforced at spec validation time

### Trade-offs

| Advantage | Disadvantage |
|-----------|--------------|
| ✅ Version control friendly | ❌ Limited expressiveness vs. Python |
| ✅ Cross-language compatibility | ❌ Requires learning YAML/JSON syntax |
| ✅ Static validation | ❌ Complex logic needs workarounds (graph pattern) |
| ✅ Security sandboxing | ❌ No dynamic imports/plugins |

### Alternative Considered: Python DSL

**Example**:
```python
from strands_cli import Workflow, Agent, Chain

workflow = Workflow(
    agents={"researcher": Agent(prompt="Research {{ topic }}")},
    pattern=Chain(
        steps=[
            Step(agent="researcher", input="Research AI"),
        ]
    )
)
```

**Why Rejected**:
- Requires Python knowledge (limits audience)
- Harder to version control (complex diffs)
- Security risks (arbitrary code execution)
- IDE support requires complex AST parsing

---

## Why JSON Schema Draft 2020-12?

### The Decision

Strands CLI uses **JSON Schema Draft 2020-12** for workflow validation instead of Pydantic-only validation, custom validators, or no validation.

### Rationale

**1. Standardized Validation**
JSON Schema is a formal specification (RFC 8927) with:
- Precise semantics for validation rules
- Cross-language implementations (Python, JavaScript, Java, etc.)
- Community-maintained tooling (validators, generators, editors)

**2. Precise Error Reporting**
JSON Schema provides **JSONPointer** paths to exact error locations:
```bash
Schema validation error at '#/pattern/config/steps/0/agent':
'missing_agent' is not one of ['researcher', 'analyst']
```

**3. IDE Integration**
JSON Schema enables:
- Autocomplete in VS Code, IntelliJ, etc.
- Inline documentation from `description` fields
- Real-time validation as you type

**4. Documentation Generation**
Schema can be auto-converted to:
- Markdown documentation
- Interactive web forms
- OpenAPI specs (for REST APIs)

**5. Future-Proof Extensibility**
JSON Schema supports:
- Conditional validation (`if`/`then`/`else`)
- Schema composition (`allOf`, `anyOf`, `oneOf`)
- Recursive schemas (for nested patterns)

### Trade-offs

| Advantage | Disadvantage |
|-----------|--------------|
| ✅ Standardized, cross-language | ❌ More verbose than Pydantic |
| ✅ Precise error messages | ❌ Requires external validation library |
| ✅ IDE integration | ❌ Learning curve for schema syntax |
| ✅ Documentation generation | ❌ Custom validation requires `format` keywords |

### Why Draft 2020-12 Specifically?

**Features Used**:
- `$defs` for reusable subschemas (cleaner than `definitions`)
- `prefixItems` for tuple validation (better than `items` arrays)
- `unevaluatedProperties: false` for strict validation
- Better error messages in most validators

**Migration Path**:
- Draft 2020-12 is backward compatible with Draft 07
- Modern validators (jsonschema, ajv) support it natively

### Alternative Considered: Pydantic-Only Validation

**Why Rejected**:
- No cross-language support (Python-only)
- Error messages less precise (no JSONPointer)
- Harder to generate documentation from models
- Less IDE integration for YAML files

**Hybrid Approach Chosen**:
- JSON Schema validates raw YAML/JSON
- Pydantic models add runtime type safety
- Best of both worlds (validation + type hints)

---

## Why Provider Abstraction?

### The Decision

Strands CLI abstracts LLM providers (Bedrock, Ollama, OpenAI) behind a **unified `Runtime` interface** instead of exposing provider-specific APIs.

### Rationale

**1. Multi-Provider Support**
Users can switch providers without changing workflow specs:
```yaml
# Development (local Ollama)
runtime:
  provider: ollama
  model_id: llama3.1

# Production (AWS Bedrock)
runtime:
  provider: bedrock
  model_id: anthropic.claude-3-sonnet-20240229-v1:0
```

**2. Cost Optimization**
Users can test with cheap models (Ollama) and deploy with premium models (Bedrock).

**3. Provider Failover**
Future support for fallback chains:
```yaml
runtime:
  providers:
    - bedrock  # Try Bedrock first
    - openai   # Fallback to OpenAI
    - ollama   # Final fallback to local
```

**4. Vendor Independence**
No lock-in to specific LLM providers:
- AWS Bedrock today, OpenAI tomorrow, custom models next month
- No code changes required (just spec updates)

**5. Model Client Pooling**
Abstraction enables `@lru_cache` pooling for efficiency:
```python
@lru_cache(maxsize=16)
def _create_model_cached(config: RuntimeConfig) -> Model:
    """Create and cache model clients."""
    # Same config → same client instance
```

### Implementation

**Runtime Configuration**:
```yaml
runtime:
  provider: bedrock | ollama | openai
  model_id: <model_name>
  region: us-east-1  # Bedrock only
  host: http://localhost:11434  # Ollama only
```

**Provider Adapters** (`runtime/providers.py`):
- `BedrockProvider` → AWS Bedrock client
- `OllamaProvider` → Ollama HTTP client
- `OpenAIProvider` → OpenAI API client

**Unified Interface**:
```python
def create_model(runtime: Runtime) -> Model:
    """Create model client based on runtime config."""
    if runtime.provider == "bedrock":
        return BedrockModel(model_id=runtime.model_id, region=runtime.region)
    elif runtime.provider == "ollama":
        return OllamaModel(model_id=runtime.model_id, host=runtime.host)
    # ...
```

### Trade-offs

| Advantage | Disadvantage |
|-----------|--------------|
| ✅ Multi-provider support | ❌ Provider-specific features harder to expose |
| ✅ Cost optimization | ❌ Lowest-common-denominator API |
| ✅ Vendor independence | ❌ Adapter maintenance overhead |
| ✅ Model client pooling | ❌ Testing requires multiple provider setups |

### Alternative Considered: Provider-Specific Specs

**Example**:
```yaml
# Bedrock-specific spec
bedrock:
  model_id: anthropic.claude-3-sonnet-20240229-v1:0
  region: us-east-1
  agents: [...]
```

**Why Rejected**:
- No cross-provider workflows
- Harder to test (need real provider credentials)
- Vendor lock-in (spec tied to provider)

---

## Why Security-First Design?

### The Decision

Strands CLI treats **all user-provided specs as potentially malicious** and implements defense-in-depth security controls.

### Threat Model

**Attack Surface**:
1. **Template Injection**: User-controlled Jinja2 templates
2. **SSRF**: User-controlled HTTP executor URLs
3. **Path Traversal**: User-controlled artifact paths
4. **Code Execution**: User-controlled Python tool callables

**Threat Actors**:
- External users submitting workflow specs
- Malicious workflow specs from untrusted repos
- Compromised CI/CD pipelines

### Security Layers

#### 1. Template Sandboxing

**Risk**: Remote code execution via template introspection.

**Mitigation**:
- `jinja2.sandbox.SandboxedEnvironment` (no `__class__`, `__mro__`, etc.)
- Whitelisted filters only: `truncate`, `tojson`, `title`
- Cleared globals (no access to Python builtins)

**Example Attack Blocked**:
```yaml
# Attempt to execute system command
outputs:
  artifacts:
    - path: "{{ ''.__class__.__mro__[1].__subclasses__()[104].__init__.__globals__['os'].system('whoami') }}"
```
**Result**: `SecurityError: access to attribute '__class__' is unsafe`

#### 2. SSRF Prevention

**Risk**: Server-Side Request Forgery via HTTP executors.

**Mitigation**:
- Blocked URL patterns: localhost, private IPs, metadata endpoints, file://, ftp://
- Configurable allowlist/blocklist via environment variables
- Pre-execution validation (before HTTP client creation)

**Example Attack Blocked**:
```yaml
tools:
  http_executors:
    - base_url: "http://169.254.169.254/latest/meta-data/"  # AWS metadata
```
**Result**: `CapabilityError: HTTP URL blocked (SSRF attempt)`

#### 3. Path Traversal Protection

**Risk**: File overwrite and directory escape via artifact paths.

**Mitigation**:
- Reject absolute paths
- Block `..` path components
- Sanitize path components (remove special chars)
- Validate resolved path (relative_to check)
- Block symlinks (MVP restriction)

**Example Attack Blocked**:
```yaml
outputs:
  artifacts:
    - path: "../../etc/passwd"
```
**Result**: `ArtifactError: Path traversal not allowed`

#### 4. Tool Allowlisting

**Risk**: Arbitrary code execution via Python callables.

**Mitigation**:
- Allowlist of safe native tools: `http_request`, `file_read`, `calculator`, etc.
- User consent prompts for dangerous operations (file_write)
- `--bypass-tool-consent` flag for CI/CD (with warnings)

**Example Attack Blocked**:
```yaml
tools:
  python:
    - callable: "os.system"  # Arbitrary code execution
```
**Result**: `CapabilityError: Python callable not in allowlist`

### Audit Logging

All security violations logged with structured fields:
```json
{
  "event": "http_url_blocked",
  "violation_type": "ssrf_attempt",
  "blocked_url": "http://169.254.169.254/...",
  "matched_pattern": "^https?://169\\.254\\.169\\.254.*$",
  "timestamp": "2025-11-06T05:03:46Z",
  "level": "warning"
}
```

### Trade-offs

| Security Benefit | Usability Cost |
|------------------|----------------|
| ✅ Template sandboxing prevents RCE | ❌ Limited filter support (only truncate/tojson/title) |
| ✅ SSRF prevention protects internal networks | ❌ Localhost blocked by default (need allowlist override) |
| ✅ Path validation prevents file overwrite | ❌ Symlinks blocked (legitimate use cases affected) |
| ✅ Tool allowlisting prevents arbitrary code | ❌ Custom tools require allowlist updates |

### Alternative Considered: Permissive by Default

**Why Rejected**:
- Security-first design is critical for enterprise adoption
- Easy to relax restrictions (allowlist override) vs. tighten later
- Audit logging provides visibility even with permissive settings

---

## Why Single Event Loop Strategy?

### The Decision

Strands CLI uses **one `asyncio.run()` per workflow** from the CLI layer. Executors use `await` internally.

### Rationale

**1. Clean Resource Management**
Single event loop enables:
- Single cleanup point (try/finally at CLI level)
- Proper HTTP client lifecycle (one session per workflow)
- No orphaned connections or file handles

**2. Async Context Propagation**
OpenTelemetry traces, structlog context, and timeouts propagate correctly:
```python
# CLI layer
with tracer.start_as_current_span("workflow.execute"):
    result = asyncio.run(execute_workflow(spec))
    # All child spans inherit this trace context
```

**3. No Nested Event Loop Issues**
Python's asyncio raises `RuntimeError` on nested `asyncio.run()` calls:
```python
# BAD: Nested event loops
asyncio.run(outer())  # CLI
    asyncio.run(inner())  # Executor - RAISES ERROR
```

Strands CLI avoids this by using `await` in executors:
```python
# GOOD: Single event loop
asyncio.run(outer())  # CLI
    await inner()  # Executor - OK
```

**4. Efficient Concurrency**
Single event loop enables:
- Semaphore-based concurrency control (`max_parallel`)
- asyncio.gather for parallel execution
- Proper task cancellation on failure

### Implementation

**CLI Layer** (`__main__.py`):
```python
@app.command()
def run(spec_file: Path, ...) -> None:
    """Execute workflow with single event loop."""
    result = asyncio.run(execute_workflow(spec_file))  # Single entry point
    sys.exit(EX_OK if result.success else EX_RUNTIME)
```

**Executor Layer** (`exec/chain.py`, etc.):
```python
async def run_chain(spec: Spec, variables: dict[str, Any]) -> RunResult:
    """Execute chain pattern (uses await, not asyncio.run)."""
    cache = AgentCache()
    try:
        for step in spec.pattern.config.steps:
            agent = await cache.get_or_build_agent(...)  # await
            result = await invoke_agent_with_retry(agent, ...)  # await
        return RunResult(...)
    finally:
        await cache.close()  # Cleanup
```

### Trade-offs

| Advantage | Disadvantage |
|-----------|--------------|
| ✅ Clean resource management | ❌ All executors must be async |
| ✅ Proper context propagation | ❌ No blocking I/O in executors |
| ✅ No nested event loop errors | ❌ Testing requires asyncio fixtures |
| ✅ Efficient concurrency | ❌ Debugging async code is harder |

### Alternative Considered: Synchronous Execution

**Why Rejected**:
- No parallelism (slower for workflow/parallel patterns)
- Harder to add concurrency later (breaking change)
- Modern Python ecosystem is async-first (httpx, strands-agents-sdk)

---

## Why Agent Caching & Model Pooling?

### The Decision

Strands CLI caches **agent instances** and **model clients** to avoid redundant initialization.

### Rationale

**1. Performance**
Without caching, a 10-step chain with the same agent config:
- Builds 10 agent instances (10× prompt compilation, tool loading)
- Creates 10 model clients (10× HTTP sessions, auth handshakes)

With caching:
- Builds 1 agent instance (reused 10 times)
- Creates 1 model client (reused 10 times)
- **~10× speedup** for multi-step workflows

**2. Resource Conservation**
HTTP clients maintain connection pools:
- Reusing clients → connection reuse (HTTP keep-alive)
- New clients → new connections (TCP handshake overhead)

**3. Consistent Behavior**
Same agent config → same agent instance → deterministic behavior:
- No subtle differences between steps
- Easier debugging (one agent to inspect)

### Implementation

**Agent Cache** (`exec/utils.py`):
```python
class AgentCache:
    """Singleton cache for agent reuse."""
    
    def __init__(self):
        self._agents: dict[tuple, Agent] = {}
        self._clients: list[Any] = []
    
    async def get_or_build_agent(
        self, spec: Spec, agent_id: str, config: AgentConfig, ...
    ) -> Agent:
        """Get cached agent or build new one."""
        cache_key = (agent_id, config_hash(config))
        if cache_key in self._agents:
            return self._agents[cache_key]  # Cache hit
        
        agent = await build_agent(spec, agent_id, config, ...)
        self._agents[cache_key] = agent
        return agent
```

**Model Client Pool** (`runtime/strands_adapter.py`):
```python
@dataclass(frozen=True)
class RuntimeConfig:
    """Hashable runtime config for LRU cache."""
    provider: str
    model_id: str
    region: str | None = None

@lru_cache(maxsize=16)
def _create_model_cached(config: RuntimeConfig) -> Model:
    """Create and cache model clients."""
    if config.provider == "bedrock":
        return BedrockModel(model_id=config.model_id, region=config.region)
    # ...
```

### Trade-offs

| Advantage | Disadvantage |
|-----------|--------------|
| ✅ 10×+ speedup for multi-step workflows | ❌ Stale agents if spec changes mid-execution |
| ✅ Resource conservation (HTTP pools) | ❌ Memory usage for cached agents |
| ✅ Deterministic behavior | ❌ Cache invalidation complexity |

### Alternative Considered: No Caching

**Why Rejected**:
- Unacceptable performance penalty for multi-step workflows
- Wasteful resource usage (new HTTP clients per step)
- No significant simplification (cache logic is ~50 lines)

---

## Why Exit Code Discipline?

### The Decision

Strands CLI uses **named exit code constants** (EX_OK, EX_SCHEMA, etc.) instead of generic `exit(1)`.

### Rationale

**1. Scriptability**
Users can handle different error types:
```bash
strands run workflow.yaml
EXIT_CODE=$?

if [ $EXIT_CODE -eq 3 ]; then
    echo "Schema error - fix workflow.yaml"
elif [ $EXIT_CODE -eq 18 ]; then
    echo "Unsupported feature - upgrade strands-cli"
fi
```

**2. Observability**
Exit codes enable:
- CI/CD pipeline branching (retry on EX_RUNTIME, fail on EX_SCHEMA)
- Metrics dashboards (count of EX_UNSUPPORTED errors)
- Alerting (page on EX_RUNTIME, ignore EX_USAGE)

**3. Consistency**
Named constants prevent:
- Arbitrary exit codes (`exit(1)`, `exit(2)`, `exit(42)`)
- Exit code collisions (two error types with same code)
- Undocumented exit codes (what does `exit(13)` mean?)

### Exit Code Map

| Code | Name | When to Use |
|------|------|-------------|
| 0 | `EX_OK` | Success |
| 2 | `EX_USAGE` | Bad CLI flags/missing file |
| 3 | `EX_SCHEMA` | JSON Schema validation error |
| 10 | `EX_RUNTIME` | Provider/model/tool runtime failure |
| 12 | `EX_IO` | Artifact write/IO error |
| 17 | `EX_SESSION` | Session not found/corrupted/already completed |
| 18 | `EX_UNSUPPORTED` | Feature present but not supported |
| 19 | `EX_HITL_PAUSE` | Workflow paused for human input (normal) |
| 20 | `EX_BUDGET_EXCEEDED` | Token or time budget exhausted |
| 70 | `EX_UNKNOWN` | Unexpected exception |

### Implementation

**Exit Code Module** (`exit_codes.py`):
```python
"""Exit codes for strands CLI."""

EX_OK = 0  # Success
EX_USAGE = 2  # Bad CLI usage
EX_SCHEMA = 3  # Schema validation error
EX_RUNTIME = 10  # Runtime error
EX_IO = 12  # I/O error
EX_SESSION = 17  # Session error
EX_UNSUPPORTED = 18  # Unsupported feature
EX_HITL_PAUSE = 19  # HITL workflow pause (normal)
EX_BUDGET_EXCEEDED = 20  # Budget exceeded
EX_UNKNOWN = 70  # Unknown error
```

**Usage in CLI**:
```python
from strands_cli.exit_codes import EX_SCHEMA, EX_OK

try:
    validate_spec(spec_data)
except SchemaValidationError as e:
    console.print(f"[red]Schema Error:[/red] {e}")
    sys.exit(EX_SCHEMA)  # NOT sys.exit(3)
```

### Trade-offs

| Advantage | Disadvantage |
|-----------|--------------|
| ✅ Scriptable error handling | ❌ Requires discipline (no `exit(1)`) |
| ✅ Observable in CI/CD metrics | ❌ Limited to 256 exit codes (not an issue) |
| ✅ Consistent across codebase | ❌ Documentation overhead |

### Alternative Considered: Generic Exit Codes

**Why Rejected**:
- No way to distinguish error types programmatically
- Harder to debug (all errors look the same)
- No CI/CD automation potential

---

## Summary

Strands CLI design decisions prioritize:

1. **Declarative Specs**: YAML/JSON for version control, security, and cross-tool compatibility
2. **Standardized Validation**: JSON Schema Draft 2020-12 for precise errors and IDE integration
3. **Provider Abstraction**: Unified runtime interface for multi-provider support and cost optimization
4. **Security-First**: Defense-in-depth for templates, HTTP, paths, and tools
5. **Single Event Loop**: Clean resource management and async context propagation
6. **Caching & Pooling**: Agent caching and model pooling for 10×+ performance gains
7. **Exit Code Discipline**: Named constants for scriptable error handling and observability

**Next Steps**: See [Architecture Overview](architecture.md), [Pattern Philosophy](patterns.md), [Performance Optimizations](performance.md), and [Security Model](security-model.md) for implementation details.
